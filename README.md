# End-to-End Data Pipeline with Iceberg, Spark, Kafka & Airflow

A complete data engineering stack demonstrating streaming and batch processing with Apache Airflow, Spark, Iceberg, Kafka, and MinIO.

## ğŸ— Architecture

### Two-Part Data Pipeline

```
                                â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                                â•‘                     ORCHESTRATION LAYER                             â•‘
                                â•‘                      Apache Airflow 3.1.6                           â•‘
                                â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                                                 â”‚
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚                â”‚                â”‚
                                                â–¼                â–¼                â–¼


                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚     STREAM PROCESSING           â”‚  â”‚    BATCH PROCESSING           â”‚
                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â”‚                                 â”‚  â”‚                               â”‚
                            â”‚  Apache Kafka 3.8.0 (KRaft)     â”‚  â”‚  Apache Airflow               â”‚
                            â”‚  â”œâ”€ Topic: iceberg_events       â”‚  â”‚  â”œâ”€ Scheduler triggers jobs   â”‚
                            â”‚  â”œâ”€ Partitions: 1               â”‚  â”‚  â”œâ”€ DAG: spark_test_dag.py    â”‚
                            â”‚  â”œâ”€ Replication: 1              â”‚  â”‚  â””â”€ Cron: On schedule         â”‚
                            â”‚  â”œâ”€ Python: kafka-python 2.3.0  â”‚  â”‚                               â”‚
                            â”‚  â””â”€ UI: Kafka UI (8088)         â”‚  â”‚  Apache Spark 3.4.1 (Batch)   â”‚
                            â”‚                                 â”‚  â”‚  â”œâ”€ Master: spark://7077      â”‚
                            â”‚  Events (JSON):                 â”‚  â”‚  â”œâ”€ Workers: 2 cores/1GB      â”‚
                            â”‚  â”œâ”€ user_signup                 â”‚  â”‚  â””â”€ Executor: LocalExecutor   â”‚
                            â”‚  â”œâ”€ purchase                    â”‚  â”‚                               â”‚
                            â”‚  â”œâ”€ page_view                   â”‚  â”‚  Batch Job:                   â”‚
                            â”‚  â””â”€ user_logout                 â”‚  â”‚  â””â”€ spark_jobs/               â”‚
                            â”‚                                 â”‚  â”‚     iceberg_write_job.py      â”‚
                            â”‚  â†“                              â”‚  â”‚                               â”‚
                            â”‚  Spark Streaming                â”‚  â”‚  Processing:                  â”‚
                            â”‚  â”œâ”€ Source: Kafka               â”‚  â”‚  â”œâ”€ Scheduled by Airflow      â”‚
                            â”‚  â”œâ”€ Format: JSON                â”‚  â”‚  â”œâ”€ Transforms batch data     â”‚
                            â”‚  â”œâ”€ Processing: Real-time       â”‚  â”‚  â””â”€ Writes to Iceberg         â”‚
                            â”‚  â””â”€ Output: Iceberg Tables      â”‚  â”‚                               â”‚
                            â”‚                                 â”‚  â”‚  Iceberg Lakehouse:           â”‚
                            â”‚  Streaming Job:                 â”‚  â”‚  â”œâ”€ Version: 1.4.3            â”‚
                            â”‚  â””â”€ (To be implemented)         â”‚  â”‚  â”œâ”€ Format: Parquet + Avro    â”‚
                            â”‚                                 â”‚  â”‚  â”œâ”€ Catalog: Hadoop (SQL)     â”‚
                            â”‚                                 â”‚  â”‚  â””â”€ Storage: S3A compatible   â”‚
                            â”‚                                 â”‚  â”‚                               â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚                                        â”‚
                                (Real-time)     â”‚                                        â”‚ (Periodic)
                                                â”‚                                        â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                 â”‚
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚                                 â”‚
                                                â–¼                                 â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚  PostgreSQL 16  â”‚          â”‚  MinIO (S3 API)  â”‚
                                        â”‚  â”œâ”€ Airflow DB  â”‚          â”‚  â”œâ”€ Endpoint:    â”‚
                                        â”‚  â””â”€ Metadata    â”‚          â”‚  â”‚   9000/9001   â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”œâ”€ Buckets:     â”‚
                                                                     â”‚  â”‚   - warehouse â”‚
                                                                     â”‚  â”‚   - raw-data  â”‚
                                                                     â”‚  â”‚   - delta-lakeâ”‚
                                                                     â”‚  â””â”€ Data: Persist| 
                                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Data Flow

**Stream Path (Real-time - Continuous):**
```
Kafka Topic (iceberg_events)
        â†“
Spark Structured Streaming Job
        â”œâ”€ Reads events in real-time
        â”œâ”€ Transforms/validates
        â””â”€ Writes to Iceberg tables (micro-batches)
        
        Output â†’ Iceberg Tables in MinIO (continuous)
```

**Batch Path (Scheduled by Airflow):**
```
Airflow DAG (spark_test_dag)
        â†“
Trigger: On schedule (or manual)
        â†“
Spark Batch Job (iceberg_write_job.py)
        â”œâ”€ Reads batch data
        â”œâ”€ Transforms/aggregates
        â””â”€ Writes to Iceberg tables
        
        Output â†’ Iceberg Tables in MinIO (periodic)
```

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- 4GB+ RAM
- 10GB+ Disk space

### Services

| Service | Port | URL |
|---------|------|-----|
| Airflow Web UI | 8082 | `http://localhost:8082` |
| Airflow API | 8082 | `http://localhost:8082/api/v1` |
| MinIO Console | 9001 | `http://localhost:9001` |
| MinIO S3 API | 9000 | `http://localhost:9000` |
| Kafka Broker | 9092 | `localhost:9092` (internal) / `localhost:9094` (host) |
| Kafka UI | 8088 | `http://localhost:8088` |
| Spark Master | 9090 | `http://localhost:9090` |

### Start All Services

```bash
docker compose up -d
```

Verify services:
```bash
docker compose ps
```

### Stop All Services

```bash
docker compose down
```

## ğŸ“Š Included DAGs & Jobs

### 1. **Kafka Producer/Consumer DAG**
**File:** `dags/kafka_producer_dag.py`

Demonstrates streaming data with Kafka:
- Produces sample events (user_signup, purchase, page_view, user_logout)
- Consumes and displays events

**Trigger:**
```bash
docker compose exec airflow-scheduler airflow dags trigger kafka_producer_dag
```

**Expected Output:**
```
âœ“ Event 1 -> topic: iceberg_events, partition: 0, offset: 0
âœ“ Event 2 -> topic: iceberg_events, partition: 0, offset: 1
...
âœ“ Successfully produced 4 events to Kafka topic 'iceberg_events'
âœ“ Consumed 4 events from Kafka
```

### 2. **Spark + Iceberg Write DAG**
**File:** `dags/spark_test.py`

Writes Iceberg tables to MinIO:
- Task 1: `print_spark_info` - Check Spark cluster
- Task 2: `submit_spark_job` - Run test Spark job
- Task 3: `write_iceberg_to_minio` - Write data to Iceberg format

**Spark Job:** `spark_jobs/iceberg_write_job.py`

**Features:**
- KRaft mode (no ZooKeeper)
- Automatic bucket creation
- Schema validation
- Table versioning
- Metadata tracking

**Trigger:**
```bash
docker compose exec airflow-scheduler airflow dags trigger spark_test_dag
```

**Expected Output:**
```
âœ“ Bucket 'test-bucket' created
âœ“ Sample DataFrame created
âœ“ Data successfully written to local.spark_demo.products
âœ“ Reading back data from Iceberg table
```

## ğŸ”§ Configuration

### Environment Variables (`.env`)

```env
AIRFLOW_UID=50000
AIRFLOW_API_BASE_URL=http://localhost:8082
AIRFLOW_WEBSERVER_BASE_URL=http://localhost:8082
MINIO_ENDPOINT=http://minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=test-bucket
```

### Airflow Credentials

- **Username:** airflow
- **Password:** airflow

### MinIO Credentials

- **Access Key:** minioadmin
- **Secret Key:** minioadmin

## ğŸ“š Key Technologies

### Kafka (KRaft Mode)
- **Version:** 3.8.0 (Apache Kafka)
- **Mode:** KRaft (no ZooKeeper required)
- **Partitions:** 1
- **Replication Factor:** 1
- **Python Client:** `kafka-python` 2.3.0

### Spark
- **Version:** 3.4.1
- **Mode:** Local[2] (2 cores)
- **Iceberg Version:** 1.4.3 (compatible with Spark 3.4)
- **Hadoop AWS:** 3.3.4 (S3A support)

### Iceberg
- **Version:** 1.4.3
- **Catalog Type:** Hadoop (file-based with SQL)
- **Storage:** S3A compatible (MinIO)
- **Format:** Parquet (data) + Avro (metadata)

### MinIO
- **Version:** Latest
- **Mode:** Standalone
- **Data Location:** `/minio_data/` (persisted volume)

### Airflow
- **Version:** 3.1.6
- **Executor:** LocalExecutor
- **Database:** PostgreSQL 16
- **Base Image:** Apache Airflow official

## ğŸ“¦ Data Storage

### MinIO Bucket Structure

```
test-bucket/
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ spark_demo/
â”‚       â””â”€â”€ products/
â”‚           â”œâ”€â”€ data/
â”‚           â”‚   â”œâ”€â”€ 00000-*.parquet
â”‚           â”‚   â””â”€â”€ 00001-*.parquet
â”‚           â””â”€â”€ metadata/
â”‚               â”œâ”€â”€ v1.metadata.json
â”‚               â”œâ”€â”€ *.avro
â”‚               â””â”€â”€ version-hint.text
â”œâ”€â”€ raw-data/
â”œâ”€â”€ delta-lake/
â””â”€â”€ processed-data/
```

## ğŸ”Œ Kafka Topics

**Topic:** `iceberg_events`
- **Partitions:** 1
- **Schema:** JSON
- **Sample Event:**
  ```json
  {
    "event_id": 1,
    "event_type": "user_signup",
    "user_id": 101,
    "timestamp": "2026-02-04T15:07:00",
    "data": {
      "email": "user1@example.com",
      "country": "US"
    }
  }
  ```

## ğŸ§ª Testing

### 1. Check Kafka Health

```bash
docker compose exec kafka /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092
```

### 2. Produce Test Message

```bash
printf "test-message\n" | docker compose exec -T kafka /opt/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic iceberg_events
```

### 3. Consume Messages

```bash
docker compose exec -T kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic iceberg_events --from-beginning --max-messages 1
```

### 4. List Iceberg Tables in MinIO

```bash
docker compose exec airflow-scheduler python -c "
import boto3
s3 = boto3.client('s3', endpoint_url='http://minio:9000', aws_access_key_id='minioadmin', aws_secret_access_key='minioadmin')
response = s3.list_objects_v2(Bucket='test-bucket', Prefix='warehouse/', MaxKeys=50)
for obj in response.get('Contents', []):
    print(obj['Key'])
"
```

## ğŸ› Troubleshooting

### Kafka Connection Issues

```
"advertised.listeners listener names must be equal to or a subset of the ones defined in listeners"
```

**Solution:** Ensure `KAFKA_LISTENERS` includes all listener types in `KAFKA_ADVERTISED_LISTENERS`.

### Spark Iceberg Version Mismatch

```
"ClassNotFoundException: org.apache.spark.sql.catalyst.expressions.AnsiCast"
```

**Solution:** Use Iceberg 1.4.3 for Spark 3.4.1 (not 1.4.2).

### MinIO Bucket Not Found

```
"The specified bucket does not exist (NoSuchBucket)"
```

**Solution:** Iceberg write job auto-creates buckets. If issue persists:
```bash
docker compose exec minio /usr/bin/mc mb minio/test-bucket
```

### Airflow DAG Not Found

Ensure DAGs are in `/dags/` directory and restart:
```bash
docker compose restart airflow-scheduler
```

## ğŸ“– Documentation Links

- [Apache Airflow](https://airflow.apache.org/)
- [Apache Kafka](https://kafka.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [Apache Iceberg](https://iceberg.apache.org/)
- [MinIO](https://min.io/)

## ğŸ¤ Contributing

To extend this pipeline:

1. **Add new DAGs:** Place Python files in `dags/`
2. **Add Spark jobs:** Place scripts in `spark_jobs/`
3. **Install packages:** Update `requirements.txt` and rebuild
4. **Add services:** Update `docker-compose.yaml`
## Current Bug

The api server on http://localhost:8082/api/v2/dags is currently unavailble

## ğŸ“ License

Apache License 2.0
